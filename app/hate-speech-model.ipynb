{"cells":[{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import math\n","import spacy\n","import pickle\n","import string\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#stop_words = spacy.lang.pt.stop_words.STOP_WORDS\n","punctuations = string.punctuation\n","nlp = spacy.load('pt_core_news_sm')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def removeChars(dataFrame):\n","    # Remove @ tags\n","    dataFrame.txt = dataFrame.txt.str.replace(r'(@\\w*)', '', regex=True)\n","\n","    # Remove URL\n","    dataFrame.txt = dataFrame.txt.str.replace(r\"http\\S+\", \"\", regex=True)\n","\n","    # Remove # tag\n","    dataFrame.txt = dataFrame.txt.str.replace(r'#\\w+', \"\", regex=True)\n","    # comp_df.tweet = comp_df.tweet.str.replace(r'#+',\"\")\n","\n","    # Remove all non-character\n","    # comp_df.tweet = comp_df.tweet.str.replace(r\"[^a-zA-Z ]\",\"\")\n","\n","    # Remove extra space\n","    dataFrame.txt = dataFrame.txt.str.replace(r'( +)', \" \", regex=True)\n","    dataFrame.txt = dataFrame.txt.str.strip()\n","\n","    # Change to lowercase\n","    dataFrame.txt = dataFrame.txt.str.lower()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def spacy_tokenizer(sentence):\n","\n","    # Creating our token object, which is used to create documents with linguistic annotations.\n","    mytokens = nlp(sentence)\n","\n","    # Lemmatizing each token and converting each token into lowercase\n","    \n","    mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n","    #mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n","\n","    # Removing stop words\n","    #mytokens = [word for word in mytokens if word not in stop_words]\n","\n","    # Removing punctuations\n","    mytokens = [str(word) for word in mytokens if str(word) not in punctuations]\n","    # return preprocessed list of tokens\n","    return mytokens\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["insults_df = pd.read_json('../dataset/df_dataset.json')\n","\n","removeChars(insults_df)\n","insults_df['corpus'] = [spacy_tokenizer(text) for text in insults_df.txt]\n","insults_df.corpus = insults_df.apply(lambda x: \" \".join(x.corpus), axis=1)\n","insults_df['label'] = insults_df.has_anger.replace('S', 1)\n","insults_df['label'] = insults_df.has_anger.replace(math.nan, 0)\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["freq_vector = CountVectorizer(min_df=2, ngram_range=(1, 2)).fit(insults_df.corpus)\n","x_train = freq_vector.transform(insults_df.corpus)\n","y_train = insults_df.has_anger"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"'<' not supported between instances of 'NoneType' and 'str'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Thiag\\Documents\\Git Projects\\hate-speech-lambda-function\\app\\hate-speech-model.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thiag/Documents/Git%20Projects/hate-speech-lambda-function/app/hate-speech-model.ipynb#ch0000009?line=0'>1</a>\u001b[0m classifier \u001b[39m=\u001b[39m LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thiag/Documents/Git%20Projects/hate-speech-lambda-function/app/hate-speech-model.ipynb#ch0000009?line=2'>3</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n","File \u001b[1;32mc:\\Users\\Thiag\\Documents\\Git Projects\\hate-speech-lambda-function\\enviroment\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1146\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1136\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[0;32m   1138\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m   1139\u001b[0m     X,\n\u001b[0;32m   1140\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39msolver \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msag\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msaga\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1145\u001b[0m )\n\u001b[1;32m-> 1146\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n\u001b[0;32m   1149\u001b[0m multi_class \u001b[39m=\u001b[39m _check_multi_class(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_class, solver, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_))\n","File \u001b[1;32mc:\\Users\\Thiag\\Documents\\Git Projects\\hate-speech-lambda-function\\enviroment\\lib\\site-packages\\sklearn\\utils\\multiclass.py:192\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_classification_targets\u001b[39m(y):\n\u001b[0;32m    181\u001b[0m     \u001b[39m\"\"\"Ensure that target y is of a non-regression type.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[39m    Only the following target types (as defined in type_of_target) are allowed:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m        Target values.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     y_type \u001b[39m=\u001b[39m type_of_target(y, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\n\u001b[0;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultilabel-sequences\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    199\u001b[0m     ]:\n\u001b[0;32m    200\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown label type: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n","File \u001b[1;32mc:\\Users\\Thiag\\Documents\\Git Projects\\hate-speech-lambda-function\\enviroment\\lib\\site-packages\\sklearn\\utils\\multiclass.py:335\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    332\u001b[0m     _assert_all_finite(y, input_name\u001b[39m=\u001b[39minput_name)\n\u001b[0;32m    333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcontinuous\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m suffix\n\u001b[1;32m--> 335\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39;49munique(y)) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m) \u001b[39mor\u001b[39;00m (y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(y[\u001b[39m0\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    336\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m suffix  \u001b[39m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n","File \u001b[1;32mc:\\Users\\Thiag\\Documents\\Git Projects\\hate-speech-lambda-function\\enviroment\\lib\\site-packages\\numpy\\lib\\arraysetops.py:272\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    270\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[0;32m    271\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts)\n\u001b[0;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    275\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Thiag\\Documents\\Git Projects\\hate-speech-lambda-function\\enviroment\\lib\\site-packages\\numpy\\lib\\arraysetops.py:333\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    331\u001b[0m     aux \u001b[39m=\u001b[39m ar[perm]\n\u001b[0;32m    332\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 333\u001b[0m     ar\u001b[39m.\u001b[39;49msort()\n\u001b[0;32m    334\u001b[0m     aux \u001b[39m=\u001b[39m ar\n\u001b[0;32m    335\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(aux\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mbool_)\n","\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'str'"]}],"source":["classifier = LogisticRegression(max_iter=500)\n","\n","classifier.fit(x_train, y_train)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"head not found","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Thiag\\Documents\\Git Projects\\hate-speech-lambda-function\\app\\hate-speech-model.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thiag/Documents/Git%20Projects/hate-speech-lambda-function/app/hate-speech-model.ipynb#ch0000010?line=0'>1</a>\u001b[0m x_train\u001b[39m.\u001b[39;49mhead()\n","File \u001b[1;32mc:\\Users\\Thiag\\Documents\\Git Projects\\hate-speech-lambda-function\\enviroment\\lib\\site-packages\\scipy\\sparse\\_base.py:764\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetnnz()\n\u001b[0;32m    763\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 764\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[1;31mAttributeError\u001b[0m: head not found"]}],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.1 ('enviroment': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"b9f52ec7db61ce3b879f20cfffe9f77e115052527b3f7fabeec0ee0a302ea680"}}},"nbformat":4,"nbformat_minor":2}
